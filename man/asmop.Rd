% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/asmop.R
\name{asmop}
\alias{asmop}
\alias{ampelt}
\title{Approximate Subset Multivariate Optimal Partitioning}
\usage{
asmop(data, alpha = 2 * log(nrow(data)), beta = 2 * log(ncol(data)) *
  log(nrow(data)), min.dist = 2, cost.func = "norm.meanvar.seglen",
  window.size, hard.restrict = TRUE, class = TRUE, verbose = FALSE)
}
\arguments{
\item{data}{An \code{n} x \code{p} matrix representing a length \code{n} time series containing observations of \code{p} variables.}

\item{alpha}{The variable-specific penalty, used to penalise the addition of a given changepoint into a given variable. A non-negative numeric value.}

\item{beta}{The multivariate penalty, used to penalise the addition of a changepoint into the model. A non-negative numeric value.}

\item{min.dist}{The minimum distance allowed between any two changepoints. Required to have an integer value of at least 2.}

\item{cost.func}{The name of the (global) cost function used by the method, given as a string. See details for possible values.}

\item{window.size}{The size of the window considered to the left and right of a given changepoint when performing subset restriction. A non-negative integer.}

\item{hard.restrict}{Logical. If \code{TRUE} then hard subset restriction is used. If \code{FALSE} then soft subset restriction is used.}

\item{class}{Logical. If \code{TRUE} then an object of class \code{cptmv} is returned.}

\item{verbose}{Logical. If \code{TRUE} then information regarding the changepoint vector check-list is printed during the algorithm.}
}
\value{
If \code{class=TRUE} then an object of S4 class \code{cptmv} is returned. The slot \code{cpts} contains the changepoints that are returned. Otherwise, if \code{class=FALSE}, a list is returned containing the following elements:

   \item{data.set}{The data set being analysed for changepoints.}
   \item{cost.func}{The name of the function used to calculate the cost.}
   \item{cpt.type}{The type of changes which are being detected, e.g. mean, mean and variance.}
   \item{alpha}{The value of the alpha penalty used.}
   \item{beta}{The value of the beta penalty used.}
   \item{num.cpt.vecs}{The number of changepoint vectors within the search-space considered.}
   \item{cpt.vecs}{A matrix containing the optimal changepoint vectors for the series.}
   \item{like}{The value of the likelihood for the optimal set of changepoint vectors.}
   \item{cpts}{The optimal changepoint locations in the series.}
   \item{subsets}{A logical matrix containing the optimal affected variable subsets for each of the detected changepoints.}
   \item{runtime}{The running time of the algorithm, in seconds.}
}
\description{
A method which implements the Approximate Subset Multivariate Optimal Partitioning (A-SMOP) algorithm. This algorithm is capable of detecting the presence of changepoints in a multivariate time series, and identifies which of the variables are affected for each detected change.
}
\details{
This method implements the Approximate Subset Multivariate Optimal Partitioning (A-SMOP) algorithm of B. Pickering [2016]. This algorithm obtains the changepoint locations within a multivariate time series, and identifies the subsets of variables which are affected by each corresponding change. This is done via the minimisation of a penalised cost function using dynamic programming.

A range of different cost functions and penalty values can be used. The use of hard restriction provides a faster but more approximate solution, whereas soft restriction is more accurate but requires more computation. Note that using soft restriction can become very slow even for moderate \code{p}, for practical purposes we recommend using hard restriction.

Values currently supported for the cost function \code{cost.func} include:
\tabular{ll}{
   \code{"norm.mean"} \tab Used for detecting changes in mean in Normally-distributed data. Assumes fixed variance parameters (\code{= 1}). The mean parameters are set to their maximum likelihood estimates. \cr
   \code{"norm.var"} \tab Used for detecting changes in variance in Normally-distributed data. Assumes fixed mean parameters (\code{= 0}). The variance parameters are set to their maximum likelihood estimates. \cr
   \code{"norm.meanvar"} \tab Used for detecting changes in both mean and variance in Normally-distributed data. The mean and variance parameters are set to their maximum likelihood estimates. \cr
   \code{"norm.mean.seglen"}, \code{"norm.var.seglen"}, \code{"norm.meanvar.seglen"} \tab Identical to \code{"norm.mean"}, \code{"norm.var"} and \code{"norm.meanvar"}, respectively, except these contain an additional log(segment length) penalty term in the likelihood for each variable. Designed for use when using the modified BIC penalty (Zhang and Siegmund, 2007) to penalise changes. \cr
}
}
\examples{
# Smaller example: Normal data, single change in mean at mid-point in 2/3 variables
n = 20; p=3
set.seed(100)
data = matrix(NA, n, p)
data[,1] = c( rnorm(n/2, 0, 1), rnorm(n/2, 10, 1) )
data[,2] = c( rnorm(n/2, 0, 1), rnorm(n/2, 10, 1) )
data[,3] = rnorm(n, 0, 1)
alpha = 2*log(n)
beta = 2*log(p)*log(n)
cost.func = "norm.mean.seglen"
window.size = 2
hard.restrict = TRUE
asmop.results = asmop(data=data, alpha=alpha, beta=beta, cost.func=cost.func, window.size=window.size, hard.restrict=hard.restrict)

# Larger example: Normal data, multiple changes in variance
data("var.change.ex")
#plot.ts(var.change.ex, nc=1)
n = nrow(var.change.ex) # 500
p = ncol(var.change.ex) # 6
asmop.results.hard = asmop(data=var.change.ex, alpha=2*log(n), beta=2*log(p)*log(n), cost.func="norm.var.seglen", window.size=10, hard.restrict=TRUE)
# WARNING: Using soft restriction (below) can take a few minutes.
#asmop.results.soft = asmop(data=var.change.ex, alpha=2*log(n), beta=2*log(p)*log(n), cost.func="norm.var.seglen", window.size=10, hard.restrict=FALSE) # Provides a better segmentation compared to hard.
}
\seealso{
\code{\link{smop}}
}
